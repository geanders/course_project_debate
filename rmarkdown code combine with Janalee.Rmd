---
title: "Debate Project"
author: "Scott Kelleher, Tulsi Gompo, Janalee Thompson"
date: "November 30, 2016"
output: html_document
---

##**2016 Presidential Debate: Content Breakdown**

  The 2016 presidential debates unfolded slowly as November approached. We watched
candidates battle to the bittersweet end trying to convince the American people that
they were the better-suited candidate to lead our country. According to CNN.com, over
84 million viewers tuned in to watch Hillary Clinton and Donald Trump debate. There
were three debates in total spanning over a couple months. Some would consider these
debates amusing (or scary) to say the least, such that terminology never before used in
political discussions was broadcasted live for United States citizens and other
countries to see in all its' glory. "Piggy" and "deplorable" were just a couple
words that were projected during the debates that caught the attention of some viewers.
So what is it that we all do when we want to know more about something? We just "Google
it" of course. 

#**What were we thinking?**

  As each debate unfolded, we wondered how many other people (like us) were using
Google to search for termininology used in the debates- like the second amendment for
example. Thus, we generated an analysis utilizing debate content and Google search
terms. We started checking out Google Trends and tried trending some search terms
ourselves. Google Trends is available to the general public with the purpose of
displaying global search trends in Google. Google Trends adjusts Google searches by
dividing each individual data point by total searches, geography and time range. In
other words, this displays the term that was searched in relation to global
search-volume. Trends in the data can be graphed by country or city. 
  
  Terms such as “second amendment” and “Isis” were some of the first terms we tried
that we knew would probably prompt some pretty drastic search trends, which they did.
Then, after using Google Trends to search what was intended to be a few words, we ended
up searching a copious amount of terms (related and unrelated to the debates)! It was
time to get down to business and locate the debate scripts and take some terms used by
our candidates for a spin in Google Trends. 

  We read through each debate script and noted some terminology that stood out.
Displayed below are some terms used by our now President-Elect Donald Trump during the
second debate. The terms `Melania`, `e-mail scandal`, `bigly`, `nasty woman`, and
`benghazi' were searched using Google during and immediately following the second
debate. "Bigly" was misinterpreted as the Donald Trump phrase, "Big League." As a
result of this confusion, Google Trends reported that "Bigly" had become the top
trending search on all of Google. Check out the Tweet
[here](http://www.aol.com/article/news/2016/10/19/1-word-most-searched-term-during-pres
idential-debate/21587735/). 

**Five Phrases that had particularly interesting Google search trends with respect to
their debate**

```{r echo = FALSE, include = FALSE, message = FALSE, results = "hide"}

## specify inputs
  Speaker <- "Hilary_Clinton"  # enter Donald_Trump or Hilary_Clinton
  Debate = "Second_Debate"   # enter First_Debate or Second_Debate or Third_Debate
  google_these_words = c("Melania", "email scandal", "Rosie O'Donnell", "nasty woman", "benghazi")  # enter up to five phrases to google in gtrends and results
  # enter one phrase you would like to see broken down by state
  google_states = c("Clinton Foundation")

##Loading libraries
library(rvest)
#library(tidyverse)
library(stringr)
library(tidytext)
library(gtrendsR)
library(tokenizers)
library(dplyr)
library(tm)
library(SnowballC)
library(wordcloud)
library(sentimentr)
library(lubridate)
library(ggplot2)
library(readr)
library(gtrendsR)
library(xml2)
library(shiny)
library(devtools)
ls("package:gtrendsR")
library(RTextTools)
library(googleVis)
library(syuzhet)
source("functions/classify_emotion.R")

library(RTextTools)
library(googleVis)
library(DT)
library(choroplethr)
library(choroplethrMaps)
library(syuzhet)

usr <- ("535rprogram@gmail.com")
  psw <- ("groupproject")
  ch <- gconnect(usr, psw)

  text_debate1 <- read_html("http://www.presidency.ucsb.edu/ws/index.php?pid=118971") # load the first debate page
  text_debate2 <- read_html("http://www.presidency.ucsb.edu/ws/index.php?pid=119038")  # load the second debate page
  text_debate3 <- read_html("http://www.presidency.ucsb.edu/ws/index.php?pid=119039")# load the third debate page 
  
  text_debate1 <- html_nodes(text_debate1, ".displaytext") %>% # isloate the text
    html_text() # get the text
  
  text_debate2 <- html_nodes(text_debate2, ".displaytext") %>% # isloate the text
    html_text() # get the text
  
  text_debate3 <- html_nodes(text_debate3, ".displaytext") %>% # isloate the text
    html_text() # get the text
  
  db1 <- ymd_hms(160927020000, tz = "UTC")
  db2 <- ymd_hms(161009020000, tz = "UTC")
  db3 <-ymd_hms(161029020000, tz = "UTC")
  date_time <-c(db1, db2, db3)
```

  Additionally, we were interested in specific terminology searched in Google at the
state level. Specifically, which states were Googling more particular terms than
others? The following graphic is categorized by state using Google Trends search term
data. This time we used the search term "Clinton Foundation" and attained the following
result. 

```{r echo=FALSE, message = FALSE, error= FALSE, warning = FALSE}
  
if(Debate=="First_Debate"){text <- text_debate1
  words_t <-c("trump")
  words_c <- c("clinton")}
  if (Debate=="Second_Debate"){text<- text_debate2
  words_t <- c("trump")
  words_c <- c("clinton")}
  if (Debate=="Third_Debate"){text<- text_debate3
  words_t <- c("trump")
  words_c <- c("clinton")}

  ##Getting the chunks of text and assigning the speaker, this just defines a function and we can place this actual code somewhere else later as long as we call the function 
  getLines <- function(person){
    
    id <- unlist(stringr::str_extract_all(text, "[A-Z]+:")) # get the speaker
    Lines <- unlist(strsplit(text, "[A-Z]+:"))[-1]  # split by speaker (and get rid of a pesky empty line)
    Lines[id %in% person] # retain speech by relevant speaker
  }
  
  ##Creating an object called debate_lines that has two parts, one for clinton and one for trump
  debate_lines <- lapply(c("CLINTON:", "TRUMP:"), getLines) 
  
  
  ##Created two separate objects with "chunks" of text in each one, one for clinton and one for trump
  
  clinton_lines <- debate_lines[1] 
  trump_lines <- debate_lines[2] 
  
  #break into clinton lines
  text_clinton <- data_frame(text_c = clinton_lines[[1]]) 
  text_clinton 
  
  #break into trump lines 
  text_trump <- data_frame(text_t = trump_lines[[1]]) 
  text_trump 
  
  ##Breaking trump lines into individual words
  trump_words <- text_trump %>%
    unnest_tokens(word, text_t)  
  #trump_words  
  
  
  #filter out the stopwords, add which debate it was and the day and time
  trump_words_unique <- !(trump_words$word %in% stopwords())
  trump_words <- mutate(trump_words, elim = trump_words_unique)
  trump_words<- filter(trump_words, trump_words$elim == TRUE)
  trump_words <- select(trump_words, -elim)
  
  ##Breaking clinton lines into individual words
  clinton_words <- text_clinton %>%
    unnest_tokens(word, text_c)  
  #clinton_words
  
  #filter out the stopwords, add which debate it was and what day and time
  clinton_words_unique <- !(clinton_words$word %in% stopwords())
  clinton_words <- mutate(clinton_words, elim = clinton_words_unique)
  clinton_words<- filter(clinton_words, clinton_words$elim == TRUE)
  clinton_words <- select(clinton_words, -elim)
  
  ##Counting the number of times each word is said for clinton
  clinton_word_frequency <- table(clinton_words)
  
  ##Counting the number of times each word is said for trump
  trump_word_frequency <- table(trump_words)
  
  ##Object that has word frequencies sorted from least to most for clinton
  clinton_most_words <- sort(clinton_word_frequency)
  
  ##Object that has word frequencies sorted from least to most for trump
  trump_most_words <- sort(trump_word_frequency)
  
  # reactive({
  #   
  #   if(input$Speaker== "Donald_Trump" & input$Debate == "First_Debate"){google_words <- c("Mexico","piggy","sophisticated","tremendously","renegotiate","Secretary","Michigan","NAFTA","red tape","bureaucratic","onerous","braggadocious","squander",
  #                                                                                         "tunnels","bridges","airport security","shooting","vigilant","Lester Holt","Hisapnic","Bloomberg","lawsuit","African American","Cyber","Doable")}
  #   else if (input$Speaker=="Hilary_Clinton" & input$Debate == "First_Debate"){google_words <- c("adversaries","abyss","commander in chief","Profit sharing","transactions","recession","superpower","hoax","perpetrated","ISIS",
  #                                                                                                "repartriation","trickle-down","bait and switch","prohibition","dishwashers","Charlotte","Vibrancy","touted","Kurdish","Caliphate","Al Qaida","Propagada")}
  #   
  #   else if (input$Speaker=="Donald_Trump" & input$Debate == "Second_Debate"){google_words <- c("make America great again","Obamacare", "terrorist state","trade deficit","Inner cities","locker room talk","ISIS chopping off heads","Medieval times","respect for women","Kathey Shelton","Paula Jhones",
  #                                                                                               "own an apology","Wikileaks","Deborah Wasserman Schultz","super delegates","special prosecuter","clinton email scandal","daughter's wedding",
  #                                                                                               "United States congress","monopolies","Johnathan Gurber","Islamphobia",
  #                                                                                               "Trojan Horse","San baernandino","Radical Islamic terrorist","Muslim ban","extreme vetting",
  #                                                                                               "criminal illegal aliens","Warren buffett","massive co-operation","Hillary as a senator","George soros","disaster as senator","Libya with gadhafi","Raqqua and Mosul","Sex Tape","people deplorable","Ambassador Steves",
  #                                                                                               "EPA killing companies","20 trillion debt")}
  #   
  #   else if (input$Speaker=="Hilary_Clinton" & input$Debate == "Second_Debate"){google_words <- c("Michelle Obama","preschool education","Captain Khan","personal emails","prescription drugs","health Insurance","Small businessman","our grandchildren",
  #                                                                                                 "Donald about women","embarras women on TV and twitter","targeted immigrants","gold star family",
  #                                                                                                 "demagogic rhetoric","American Muslims","Homeland security","violent Jihadist terrorist",
  #                                                                                                 "religious freedom and liberty","terrorist sites","intelligence community","Entanglement","foster care system",
  #                                                                                                 "Bipartisan worker","Syria is catastrophic","aggressiveness of Russia","American Forces in syria","Kurdish Peshmerga fighters","Children defense fund","Texas registering Latinos",
  #                                                                                                 "Great Depression","Wealthy American","Supreme court","Second Amendment","Gun show loopholes","Donald children")}
  #   
  #   else if (input$Speaker=="Donald_Trump" & input$Debate == "Third_Debate"){google_words <-  c("aleppo","African American rights","aligned","amendment","amnesty","arabia","beneficiary","border security","businesses","campaign","catastrophe",
  #                                                                                               "china company","congressional","corrupt","crosstalk","depreciation","gdp","ginsburg","gun","haiti","hombres", "humanitarian", "illegal immigrants","imperative","isis","islamic terrorist",
  #                                                                                               "Hilary for jail","unemployment","latinos","macarthur","mexico","migration","nafta","nasty woman","nightmare Trump")}
  #   
  #   else if (input$Speaker=="Hilary_Clinton" & input$Debate == "Third_Debate"){google_words <- c("aleppo","alicia","apprenticeships","Citizenship","Chinese","Catastrophic","Crocodile","Criminals","Cyberattacks","Denigrating","deportation","Discrimination","foundation","Espionage","Heartbreaking","Hurricanes","immigrants","Iranians","Jeopardy",
  #                                                                                                "Laughingstock","Lgbt","mischaracterization","Negotiation","putin","Racketeering","Radicalization","Reagan","Recession",
  #                                                                                                "Republicans","Russian","Saudi", "Steelworkers","Sunni","terrorism","Vladimir","Wikileaks")}
  #   
  #   
  # })    
  
 
  # textr <-renderPrint({ input$text})
  # output$value <- renderPrint({ input$text })
  google_results <- gtrends(google_these_words, geo = "US", start_date = "2016-09-01", end_date = "2016-11-15")
   plot(google_results)
   
   google_breakdown <- gtrends(google_states, geo = "US", start_date = "2016-09-01", end_date = "2016-11-15")
   
  by_state <-google_breakdown[[5]]
  by_state$Subregion <- tolower(by_state$Subregion)
  by_state <- rename(by_state, c(Subregion = "region", `United.States` = "value"))
   # rename(region = subregion)
    
  state_choropleth(by_state)
  
  #should be filtered down to which candidate and which debate at this point
  #some_clinton_words <- gtrends(c("women", "undocumented", "security", "espionage"), geo = "US", start_date = "2016-09-01", end_date = "2016-11-15")
  #plot(some_clinton_words)
  
  if(Speaker == "Donald_Trump"){top_used_words <- trump_most_words}
  if(Speaker =="Hilary_Clinton"){top_used_words<- clinton_most_words}

 DT::datatable(as.data.frame(top_used_words), options = list(pageLength = 25))
  
  plot(top_used_words)
  
  # https://www.r-bloggers.com/intro-to-text-analysis-with-r/
  
  #library(RTextTools)
  
  #install.packages("devtools")
  #require(devtools)
  #data <- readLines("https://www.r-bloggers.com/wp-content/uploads/2016/01/vent.txt") # from: http://www.wvgazettemail.com/
  
  # the below function was developed from 
  #http://www.rdatascientists.com/2016/08/intro-to-text-analysis-with-r.html
  
  if(Speaker == "Donald_Trump"){
    lines_go <- trump_lines
  } else if(Speaker =="Hilary_Clinton"){
    lines_go <- clinton_lines} 
 #  
 #  df <- data.frame(lines_go)
 #  colnames(df) <- c("col1") 
 #  textdata <- df[df$col1, ] 
 #  textdata = gsub("[[:punct:]]", "", textdata)  
 #  
 #  textdata = gsub("[[:punct:]]", "", textdata)
 #  textdata = gsub("[[:digit:]]", "", textdata)
 #  textdata = gsub("http\\w+", "", textdata)
 #  textdata = gsub("[ \t]{2,}", "", textdata)
 #  textdata = gsub("^\\s+|\\s+$", "", textdata)
 #  try.error = function(x)
 #  {
 #    y = NA
 #    try_error = tryCatch(tolower(x), error=function(e) e)
 #    if (!inherits(try_error, "error"))
 #      y = tolower(x)
 #    return(y) 
 #  }
 #  textdata = sapply(textdata, try.error)
 #  textdata = textdata[!is.na(textdata)]
 #  names(textdata) = NULL
 #  
 #  class_emo = classify_emotion(textdata, algorithm="bayes", prior=1.0)
 #  emotion = class_emo[,7]
 #  emotion[is.na(emotion)] = "unknown"
 # 
 #  class_pol = classify_polarity(textdata, algorithm="bayes")
 #  polarity = class_pol[,4]
 #  
 #  
 #  sent_df = data.frame(text=textdata, emotion=emotion,
 #                       polarity=polarity, stringsAsFactors=FALSE)
 #  sent_df = within(sent_df,
 #                   emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
 #  
 # # detach("package:sentiment", unload=TRUE)                
 #  
 #  # pdf(plots.pdf) 
 #  
 #    ggplot(sent_df, aes(x=emotion)) +
 #    geom_bar(aes(y=..count.., fill=emotion)) +
 #    scale_fill_brewer(palette="Dark2") +
 #    labs(x="emotion categories", y = "")
 #  # dev.off()
```

```{r echo=FALSE, message = FALSE, error= FALSE, warning = FALSE}

s_v <- get_sentences(lines_go)
s_v_sentiment <- get_sentiment(as.vector(clinton_words$word))
plot(
  s_v_sentiment, 
  type="l", 
  main="Example Plot Trajectory", 
  xlab = "Narrative Time", 
  ylab= "Emotional Valence"
  )


percent_vals <- get_percentage_values(s_v_sentiment, bins = 10)
plot(
  percent_vals, 
  type="l", 
  main="", 
  xlab = "Narrative Time", 
  ylab= "Emotional Valence", 
  col="red"
  )



```
